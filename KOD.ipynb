{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KOD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9cXg1ZUklx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bebc7827-0d52-444c-e508-27eb7bfd17a2"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# Ustawmy wersję tensorflow\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nn74wO5koNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import scipy as sp \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "import os\n",
        "# Using google.colab.drive\n",
        "from google.colab import drive\n",
        "# Using PyDrive (używamy tego wyżej)\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import glob #wypisuje lokalizacje wszystkich obrazków po kolei "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxLFK2Tks7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'projekt nn/'\n",
        "\n",
        "train_imgs = glob.glob(base_dir + \"train/*.png\")\n",
        "train_imgs.sort()\n",
        "train_cleaned_imgs = glob.glob(base_dir + \"train_cleaned/*.png\")\n",
        "train_cleaned_imgs.sort()\n",
        "test_imgs= glob.glob(base_dir + \"test/*.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlUWqd7Vkxh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bZR-TrKk3cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATCH_WIDTH_HALF = 4\n",
        "PATCH_WIDTH = PATCH_WIDTH_HALF * 2 + 1\n",
        "def train_patch_generator(train_imgs, train_cleaned_imgs, epochs = 5):\n",
        "    for _ in range(epochs):\n",
        "        print(f\"Train patch generator Epoch: {_ + 1}\")\n",
        "        for train_file, train_cleaned_file in zip(train_imgs, train_cleaned_imgs):\n",
        "            patches =[]\n",
        "            labels =[]\n",
        "            train_img = cv2.imread(train_file, cv2.IMREAD_GRAYSCALE)\n",
        "            train_cleaned_img = cv2.imread(train_cleaned_file, cv2.IMREAD_GRAYSCALE)\n",
        "            train_cleaned_img = cv2.threshold(train_cleaned_img, 200, 255,cv2.THRESH_BINARY)[1]\n",
        "            \n",
        "            train_img_ext = cv2.copyMakeBorder(train_img, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, cv2.BORDER_REPLICATE)\n",
        "           \n",
        "            for i in range(train_img.shape[0]):\n",
        "                for j in range(train_img.shape[1]):\n",
        "                    label = train_cleaned_img[i][j]\n",
        "                    patch_c1 = train_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\n",
        "                    patches.append(np.expand_dims(patch_c1, axis=2))\n",
        "                    labels.append(label / 255.)\n",
        "            patches = np.array(patches)\n",
        "            labels = np.array(labels)\n",
        "            yield (patches, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2qRcrhck92O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
        "          activation='relu', input_shape=(PATCH_WIDTH, PATCH_WIDTH, 1)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu')) #256 ilosc neuronow\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.5)) \n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(lr=0.002),\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0fjdjQ_lD95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "partial_train_imgs, validate_imgs, partial_train_labels, validate_labels = train_test_split(train_imgs, train_cleaned_imgs, test_size=0.1)\n",
        "EPOCHS=10\n",
        "model.fit_generator(train_patch_generator(partial_train_imgs, partial_train_labels, EPOCHS),\n",
        "                    epochs=EPOCHS,\n",
        "                    steps_per_epoch=len(partial_train_labels),\n",
        "                    verbose=2)\n",
        "score = model.evaluate_generator(train_patch_generator(validate_imgs, validate_labels, 1),\n",
        "                                 steps=len(validate_labels),\n",
        "                                 verbose=1)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcpZ8zEQlJ31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_patch_generator(test_imgs):\n",
        "    for test_file in test_imgs:\n",
        "        patches = []\n",
        "        test_img = cv2.imread(test_file, cv2.IMREAD_GRAYSCALE)\n",
        "        test_img_ext = cv2.copyMakeBorder(test_img, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, PATCH_WIDTH_HALF, cv2.BORDER_REPLICATE)\n",
        "        #thresholded_img_ext = cv2.adaptiveThreshold(test_img_ext,255,cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "        #                                            cv2.THRESH_BINARY,51,30) \n",
        "        #eroded_img_ext = cv2.erode(train_img_ext, np.ones((3,3),np.uint8), 1)\n",
        "        #eroded_thresh_ext = cv2.erode(thresholded_img_ext, np.ones((3,3),np.uint8), 1)\n",
        "        for i in range(test_img.shape[0]):\n",
        "            for j in range(test_img.shape[1]):\n",
        "                patch_c1 = test_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32) / 255.\n",
        "                #patch_c2 = thresholded_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255.\n",
        "                #patch_c3 = eroded_img_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255..\n",
        "                #patch_c4 = eroded_thresh_ext[i:i+PATCH_WIDTH, j:j+PATCH_WIDTH].astype(np.float32)/255..\n",
        "                patches.append(np.expand_dims(patch_c1, axis=2))\n",
        "                #patches.append(np.stack((patch_c1, patch_c2), axis=2))\n",
        "        patches = np.array(patches)\n",
        "        yield patches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRIqh6LulKzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in [8]:\n",
        "    img = cv2.imread(train_imgs[idx], cv2.IMREAD_GRAYSCALE)\n",
        "    cleaned_img = cv2.imread(train_cleaned_imgs[idx], cv2.IMREAD_GRAYSCALE)\n",
        "    predicted_mask = model.predict_generator(\n",
        "        generator=test_patch_generator([train_imgs[idx]]),\n",
        "        steps=1).reshape(img.shape).clip(0, 1).round()\n",
        "    predicted = cv2.bitwise_and(img, 255, mask=(1-predicted_mask).astype(np.uint8))\n",
        "    predicted = cv2.bitwise_or(predicted, 255, mask=predicted_mask.astype(np.uint8))\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.subplot(3,2,1)\n",
        "    plt.imshow(img, 'gray');\n",
        "    plt.title('Uncleaned')\n",
        "    plt.subplot(3,2,2)\n",
        "    plt.imshow(cleaned_img, 'gray');\n",
        "    plt.title('Manually Cleaned')\n",
        "    plt.subplot(3,2,3)\n",
        "    plt.imshow(predicted_mask.astype(np.uint8), 'gray');\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.subplot(3,2,4)\n",
        "    plt.imshow(predicted, 'gray');\n",
        "    plt.title('Auto Cleaned')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}